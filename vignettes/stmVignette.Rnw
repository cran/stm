%\VignetteIndexEntry{Using stm}
\documentclass[nojss]{jss}

\author{\hspace{1.1in}Margaret E. Roberts\\\hspace{1.1in}UCSD \And
  \hspace{1.5in}Brandon M. Stewart\\\hspace{1.5in}Harvard \And
  \hspace{1.5in}Dustin Tingley\\\hspace{1.5in}Harvard \And
}
\title{\pkg{stm}: \proglang{R} Package for Structural Topic Models}

\Plainauthor{Margaret E. Roberts, Brandon M. Stewart, Dustin Tingley} %% comma-separated
\Plaintitle{stm: R Package for Structural Topic Models} %% without formatting
\Shorttitle{Structural Topic Models}  %% a short title (if necessary)

\Abstract{
This vignette demonstrates how to use the Structural Topic Model, \pkg{stm}, \texttt{R} package. The Structural Topic Model (STM) allows researchers to estimate a topic model which includes document-level meta-data. The \pkg{stm} package provides a range of features from model selection to extensive plotting and visualization options.
}

\Keywords{structural topic model, text analysis, LDA, \pkg{stm}, \proglang{R}}
\Plainkeywords{structural topic model, text analysis, LDA, stm, R} %% without formatting

\Address{
  Margaret E. Roberts\\
  Department of Political Science\\
  University of California, San Diego\\
  Social Sciences Building 301\\
  9500 Gillman Drive, 0521, La Jolla, CA, 92093-0521 \\
  E-mail: \email{meroberts@ucsd.edu}\\
  URL: \url{http://www.margaretroberts.net}\\
  \\
  Brandon M. Stewart\\
  Department of Government\\
  Harvard University\\
  1737 Cambridge St, Cambridge, MA, USA\\
  E-mail: \email{bstewart@fas.harvard.edu}\\
  URL: \url{http://scholar.harvard.edu/bstewart}\\
\\
  Dustin Tingley\\
  Department of Government\\
  Harvard University\\
  1737 Cambridge St, Cambridge, MA, USA\\
  E-mail: \email{dtingley@gov.harvard.edu}\\
  URL: \url{http://scholar.harvard.edu/dtingley}\\

}


% == BibTeX packages
\usepackage{natbib}

% == Other Packages
\usepackage{amsmath, amsfonts, amssymb}
%, url, bm}
%\usepackage{rotating}
%\usepackage{latexsym}
%\usepackage{graphicx}
%\usepackage{ulem}


% == New Commands
% = For general typesetting
\newcommand\spacingset[1]{\renewcommand{\baselinestretch}{#1}\small\normalsize}

\begin{document}
\SweaveOpts{concordance=TRUE}
\spacingset{1.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\subsection{Method Overview}

In this vignette we demonstrate how to use the Structural Topic Model (STM), \pkg{stm}, \texttt{R} package.\footnote{We thank Antonio Coppola, Jetson Leder-Luis, Christopher Lucas, and Alex Storer for various assistance in the construction of this package.  Additional details and development version at \url{structuraltopicmodel.com}} Building off of the tradition of generative topic models, such as the Latent Dirichlet Allocation (LDA) \citep{blei2003latent} and Correlated Topic Model (CTM) \citep{blei2007correlated}, the Structural Topic Model's key innovation is that it permits users to incorporate \emph{metadata}, defined as information about each document, into the topic model.

The analysis of political data often relies on the quantification of text data. Text data is ubiquitious in social science research: traditional media, social media, survey data, and innumerable other sources contribute to the massive quantity of text in the modern information age.  The Structural Topic Model provides users a tool for machine-assisted reading of large text corpora.  With the STM, users can model anything from the American National Election Study results \citep{ajps} to Chinese newspaper articles \citep{robertsstewartchina} to online class forums \cite{StudentText} to Twitter feeds \citep{TextComparative}.

The goal of the Structural Topic Model is to enable the discovery of topics and the estimation of their relationship to document metadata. Outputs of the model can be used to conduct hypothesis testing about these relationships. This vignette illustrates use of the various components of the package. The design of the package is such that users have a broad array of options to process raw text data, explore and analyze the data and present findings utilizing a range of plotting tools.

The outline of this paper is as follows. In Section~\ref{sec:est} we introduce the technical aspects of the STM, including the data generating process and an overview of estimation.  In Section \ref{sec:use} we provide examples of how to use the model and the package \texttt{stm}, including implementing the model and plots to visualize model output.

\section{Model}
\label{sec:est}

We begin by providing a technical overview of the STM model. Later in the paper we discuss additional technical details. Like other topic models the STM is a generative model. That means we define a data generating process for each document and then use the data to find the most likely values for the parameters within the model.  The generative process for each document (indexed by $d$) can be summarized as:

\begin{enumerate}
\item Draw the document-level attention to each topic from a logistic-normal GLM based on document covariates $X_d$. \\
$\vec{\theta}_d | X_d\gamma, \Sigma \sim$ LogisticNormal$(\mu = X_d\gamma, \Sigma)$
\item Form the document-specific distribution over words representing each topic ($k$) using the baseline word distribution ($m$), the topic specific deviation $\kappa_k$, the covariate group deviation $\kappa_g$ and the interaction between the two $\kappa_i$.\\
$\beta_{d,k} \propto $ exp$(m + \kappa_{k} + \kappa_{g_d} + \kappa_{i=(kg_d)})$
\item For each word in the document, ($n \in 1, \dots, N_d$):
\begin{itemize}
\item Draw word's topic assignment based on the document-specific distribution over topics.\\
 $z_{d,n} | \vec{\theta}_d \sim $ Multinomial$(\vec{\theta})$
\item Conditional on the topic chosen, draw an observed word from that topic.\\
$ w_{d,n} | z_{d,n}, \beta_{d,k=z} \sim $ Multinomial$(\beta_{d,k=z})$
\end{itemize}
\end{enumerate}

Regularizing prior distributions are used for $\gamma, \kappa$ and (optionally) $\Sigma$ which help enhance interpretation and prevent overfitting.  To fit the model, we use a semi-collapsed variational Expectation-Maximization algorithm.  Figure~\ref{fig:stmoverview} provides a graphical representation of the model. Further details are provided in additional manuscripts \citep{nips2013,STMEdo,ajps,TextComparative}.\footnote{Available \href{http://scholar.harvard.edu/files/dtingley/files/topicmodelsopenendedexperiments.pdf}{here}, \href{http://scholar.harvard.edu/files/bstewart/files/stmnips2013.pdf}{here}, and \href{http://scholar.harvard.edu/files/dtingley/files/comparativepoliticstext.pdf}{here}.} In this vignette, we provide only brief interpretations of results, and instead direct readers to the companion papers for complete applications.

\begin{figure}
  \centering
  \includegraphics[scale=.45]{STMdiagram.pdf}
  \caption{Heuristic description of generative process and estimation of STM.}\label{fig:stmoverview}
\end{figure}

\section{Using the Structural Topic Model}
\label{sec:use}

In this section we demonstrate the basics of using the package.\footnote{The stm package depends on a variety of other packages, including \pkg{MatrixStats} \citep{MatrixStats}, \pkg{slam} \citep{slam}, \pkg{lda} \citep{lda}, \pkg{stringr} \citep{stringr}, \pkg{SnowballC} \citep{SnowballC}, \pkg{tm} \citep{meyer2008text}, \pkg{igraph} \citep{igraph}, \pkg{huge} \citep{huge}, and \pkg{glmnet} \citep{friedman2010regularization}.} Use of the STM typically proceeds in three key steps:

\begin{enumerate}
\item Reading in and processing text data \\
(\code{textProcessor, readCorpus, prepDocuments})
\item Fitting the Structural Topic Model
(\code{stm, selectModel, manyTopics})
\item Plotting and inspecting results
(\code{plotModels},\code{plot.STM},\code{labelTopics}, \code{estimateEffect}, \code{plot.estimateEffect},\code{findThoughts}, \code{plotQuote})
\end{enumerate}

Next we walk through each of these steps to show users how to use the above functions. All of the functions come with help files, and examples, that can be accessed by typing ? and then the function's name.

\subsection{Reading in textual data}

The first step is to load data into \texttt{R}. The \pkg{stm} package represents a text corpus in three parts: a \code{documents} list containing word indices and their associated counts,\footnote{A full description of the sparse list format can be found in the help file for \code{stm}.} a \code{vocab} character vector containing the words associated with the word indices and a \code{metadata} matrix containing document covariates. In this section, we describe utility functions for reading text data into \texttt{R} or converting from a variety of common formats. Note that particular care must be taken to ensure that documents and their vocabularies are properly aligned with the metadata.

<<eval=TRUE, echo=FALSE>>=
library("stm")
set.seed(02138)
@

\subsubsection{Reading in data from a ``spreadsheet''}

For purposes of example within the vignette, we will use a collection of blogposts about American politics that were written in 2008, from the CMU 2008 Political Blog Corpus \citep{poliblog}.\footnote{The set of blogs is available at \url{http://sailing.cs.cmu.edu/socialmedia/blog2008.html} and documentation on the blogs is available at \url{http://www.sailing.cs.cmu.edu/socialmedia/blog2008.pdf}.  You can find the cleaned version of the data we used for this vignette here: \url{http://goo.gl/tsprNO}.} The blogposts were gathered from six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress, and Talking Points Memo.  Each blog has its own particular political bent.  The day within 2008 when each blog was written was also recorded.  Thus for each blogpost, there is metadata on the day written and the political ideology of the blog in which it was written.

A common way that researchers store textual data alongside covariates related to the text is by having all the data within one spreadsheet, with each row a separate observation and one of the column variables the ``textual'' data field. The \pkg{stm} packages comes with a special function, \code{textProcessor}, that conveniently reads in data stored in this format and processes the data to ready it for analysis in the \pkg{stm} package.  For example, users would first read in a csv file using native R functions, or load a pre-prepared dataframe as we do below. Next, they would pass this object through the \code{textProcessor} function. This function uses a range of features from the \texttt{tm} package, such as stemming and stop word removal.

<<eval=FALSE>>=
#read in your data that is in a spreadsheet form .csv file here)
data <- read.csv("poliblogs2008.csv")
#stemming/stopword removal, etc.
processed <- textProcessor(data$documents, metadata=data)
#structure and index for usage in the stm model. Verify no-missingness.
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
#output will have object meta, documents, and vocab
docs <- out$documents
vocab <- out$vocab
meta  <-out$meta
@

After reading in the data we suggest using the utility function \code{prepDocuments()} to process the loaded data to make sure it is in the right format. In particular, the \code{prepDocuments()} function properly associates metadata with textual data, re-indexes this relationship when textual data fields are blank, or become blank following pre-processing (such as with stop word removal). Please see the help file for this function for more details.

\code{prepDocuments()} also removes infrequent terms depending on user-set parameter \code{lower.thresh}. The utility function \code{plotRemoved()} will plot the number of words and documents removed for different thresholds. For example,the user can use:

<<eval=FALSE>>=
plotRemoved(processed$documents, lower.thresh=seq(1,200, by=100))
@

to evaluate how many words and documents would be removed from the dataset at word threshold, which is the minimum number of documents a word needs to appear in in order for the word to be kept within the vocabulary.  
%The above code creates the plots below.


%\begin{figure}
%  \centering
%  \includegraphics[scale=.45]{plotRemoved_poliblogs.pdf}
%  \caption{Documents and words removed at different word thresholds.}
%\end{figure}


Importantly, \code{prepDocuments()} also will re-index all metadata/document relationships if any changes occur due to processing. After reading in and processing the textual data, it is important to inspect features of the documents and the associated vocabulary list. If users encounter problems with estimation, they should always inspect features such as these, and if using metadata, make sure the metadata has the same number of rows as there are documents. Furthermore, the model does not permit estimation when there are variables used in the model that have missing values.

From here, researchers would be ready to estimate a structural topic model.

\subsubsection{Reading in data from other text processing programs}

Sometimes researchers will encounter data that is not in a spreadsheet format. The \code{readCorpus} function includes functionality for loading data from a wide variety of formats, includomg tje standard R matrix class along with the sparse matrices from the packages \pkg{slam} and \pkg{Matrix}.  Document term matrices created by the popular package \pkg{tm} are inherited from the \pkg{slam} sparse matrix format and thus are included as a special case.

A program that is helpful for setting up and processing textual data, alongside document metadata, is \href{www.txtorg.org}{txtorg} \citep{TextComparative}. When using \pkg{txtorg}, three separate files are generated. A metadata file, a vocabulary file, and a file with the original documents. The default export format for \pkg{txtorg} is the \texttt{ldac} sparse matrix format popularized by David Blei's implementation of LDA.  The \code{readCorpus()} function can read in data of this type using the ``ldac'' option.

\subsection{Estimating the STM}

Once data is properly imported there will be documents, vocabulary and metadata that can be used for an analysis. In this subsection we illustrate how to estimate the STM.\footnote{We note that all the examples here have a very small number of maximum iterations of the EM algorithm to reduce run time, and so no substantive conclusions should be drawn.}

\subsubsection{Ways to use metadata}

The key innovation of the STM is a general approach to incorporating metadata into the topic modeling framework.  We consider two way that metadata can enter the topic model: topical prevalence and topical content.  Metadata covariates for \emph{topical prevalence} allow the observed metadata to affect the frequency with which a topic is discussed.  Covariates in \emph{topical content} allow the observed metadata to affect the word rate use within a given topic, that is, how a particular topic is discussed. Estimation for both topical prevalence and content proceeds via the workhorse \code{stm} function.

\subsubsection{Estimation with topical prevalence parameter}

In this example, we use the ratings variable (blog ideology) as a covariate in the topic \emph{prevalence} portion of the model with the CMU Poliblog data described above. Each document is modeled as a mixture of multiple topics. Topical prevalence captures how much each topic contributes to a document. Because different documents come from different sources, it is natural then to want to allow this prevalence to vary with metadata that we have about document sources.

In this example we simply let prevalence be a function of the ``ratings'' variable, which is coded as either ``Liberal'' or ``Conservative'' and the variable ``day'' which is a integer measure of days running from the first to the last day of 2008. Here a 20 topic STM is estimated. The output from the model, \code{poliblogPrevFit}, could then be passed through the various functions we discuss below (e.g., \code{plot.STM}) for inspecting the results.

If a user wishes to specify additional prevalence covariates, she would do so using the standard formula notation common in \code{R} which we discuss at greater length below. A feature of the \code{stm()} function is that ``prevalence'' can be expressed as a formula which can include multiple covariates and factorial or continuous covariates.  For example, by using the formula setup we can enter other covariates additively. Additionally users can include more flexible functional forms of continuous covariates, including standard transforms like \code{log()} etc., as well as \code{ns()} or \code{bs()} from the \pkg{splines} package. The \pkg{stm} package also includes a convenience function \code{s()} which selects a fairly flexible b-spline basis. In the current example we allow for the variable ``date'' to be estimated with a spline. As we show later in the vignette, interactions between covariates can also be added using the standard notation for \texttt{R} formulas. In summary, for the below example, we enter in the variables additively, but allowing for the day variable, an integer variable measuring which day the blog was posted, to have a non-linear relationship in the topic estimation stage.

<<eval=FALSE>>=
poliblogPrevFit <- stm(out$documents,out$vocab,K=20,
            prevalence =~ rating+ s(day), max.em.its=75,
            data=out$meta,seed=5926696)
@

The model is set to run for a maximum of 75 EM iterations (\code{max.em.its()}) using a seed we selected (\code{seed}).  Typically, convergence of the model will be monitored by the change in the approximate bound between EM iterations.  Once the bound has a small enough change between iterations, the model is considered converged. To reduce compiling time, in this vignette we do not run the models and instead load a workspace with the models already estimated.

<<eval=TRUE>>=
 load(url("http://goo.gl/91KbfS"))
@

\subsection{Model Selection and Search}

\subsubsection{Model selection for a fixed number of number of topics}
As with all mixed-membership topic models the posterior is intractable and non-convex, resulting in a multimodal estimation problem which can be sensitive to initialization. Hence users may wish to estimate a many models, each from randomly generated starting values, and then evaluate each model according to some separate standard. The function \code{selectModel} automates this process to facilitate finding a model with good properties.

Users specify the number of ``runs'', which in the example below is set to 20. \code{selectModel} first casts a net where ``run'' (below 10) models are run for two EM steps, and then models with low likelihoods are discarded. Next the default returns the 20\% of models with the highest likelihoods which are then run until convergence or the EM iteration maximum is reached. Notice how that options for the \code{stm()} function can be passed to \code{selectModels}, such as \code{max.em.its}. If users would like to select a larger number of models to be run completely, this can also be set with an option specified in the help file for this function.

<<eval=FALSE>>=
poliblogSelect <- selectModel(out$documents,out$vocab,K=20,
        prevalence =~ rating+s(day), max.em.its=75,
        data=meta,runs=20,seed=8458159)
@

In order to select a model for further investigation, users must choose from one of the candidate models output from \code{selectModel()}. To do this \code{plotModel()} can be used to plot the average \emph{semantic coherence} and \emph{exclusivity} scores for each model (represented by topic numbers) as well as the semantic coherence and exclusivity for each topic.\footnote{See \citet{STMEdo,ajps} for a discussion of these criteria.} Each of these criteria are calculated for each topic within a model run. The \code{plotModel()} function calculates the average across all topics for each run of the model and plots these by labeling the model run with a numeral. Often times users will select a model with desirable properties in both dimensions (i.e., models with average scores towards the upper right side of the plot). As shown in Figure~\ref{fig:select}, the \code{plotModel()} function also plots each topic's values which helps give a sense of the variation in these parameters.\footnote{For a given model, the user can plot the semantic coherence and exclusivity scores with the \code{topicQuality} function.}


\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plotModels(poliblogSelect)
@
\caption{Plot of selectModel results. Numerals represent the average for each model, and dots represent topic specific scores.}
\label{fig:select}
\end{center}
\end{figure}

Next the user would want to select one of these models to work with. For example, the third model could be extracted from the object that is outputted by \code{selectModel}.

<<eval=FALSE>>=
poliblogPrevFit<-poliblogSelect$runout[[3]] #choose the third model
@

\subsubsection{Model search across numbers of topics}

STM assumes a fixed user-specified numebr of topics.  A data driven approach to selecting the number of topics can be achieved by estimating using \code{stm} or the \code{selectModel} tool across multiple different numbers of topics. For example, one could estimate a STM model for 7 and 10 topics. The utility function \code{manyTopics()} allows the user to specify a range of topic numbers on which she would like to run \code{selectModel()}. \code{manyTopics()} will then iterate and run \code{selectModel()} for each different number of topics. Unlike \code{selectModel()}, this function automatically selects one run of the model. To do this, it discards runs that are strictly dominated by other model runs on both exclusivity and semantic coherence. Note that the process of extracting results differs from \code{selectModel()} because results are stored in a list environment.

<<eval=FALSE>>=
storage<-manyTopics(out$documents,out$vocab,K=c(7,10),
        prevalence =~ rating+s(day),data=meta, runs=10)
#This chooses the output, a single run of STM that was selected,
#from the runs of the 3 topic model
t<-storage$out[[1]]
#This chooses the output, a single run of STM that was selected,
#from the runs of the 4 topic model
t<-storage$out[[2]]
@

\subsection{Interpreting the STM: Plotting and inspecting results}

After choosing a model based on ex-ante criteria, the task of interpretation comes next which is especially crucial for any unsupervised procedure. There are many ways to investigate the output, ranging from inspecting the words associated with topics to the relationship between metadata and topics. To investigate the output of the model the \pkg{stm} package provides a number of options.

\begin{enumerate}
\item Displaying words associated with topics (\code{labelTopics},\code{plot.STM(,type="labels")}, \code{sageLabels},\code{plot.STM(,type="perspectives")}) or documents highly associated with particular topics (\code{findThoughts},\code{plotQuote})
\item Plotting relationships between metadata and topics/topical content (\code{estimateEffect}, \code{plot.estimateEffect})
\item Corpus level summaries (\code{plot.topicCorr},\code{plot.STM(,type="summary")})
\end{enumerate}

\subsubsection{Exploring Topics}

We next describe two ways that users can explore the topics that have been estimated. The first way is to look at collections of words that are associated with topics. The second way is to read actual documents that are estimated to be highly associated with each topic. Both of these ways should be used.

To explore the words associated with each topic we can use the \code{labelTopics()} function.  For models where a content covariate is included \code{sageLabels()} can also be used. Both these functions will print to the monitor words associated with each topic. The function by default prints several different types of word profiles, including highest probability words and FREX words.\footnote{For more information on FREX and high probability rankings, see \citet{nips2013,STMEdo,ajps,TextComparative}. For more information on score, see the LDA R package, \url{http://cran.r-project.org/web/packages/lda/lda.pdf}. For more information on lift, see \citet{taddy2012multinomial}.}  In order to translate these results to a format that can easily be used within a paper, the \code{plot.stm(,type="labels")} function will print topic words to a graphic device. Notice that in this case, the labels option is specified as the \code{plot.STM} function has several functionalities that we describe below (the options for "perspectives" and "summary").

<<eval=TRUE>>=
labelTopics(poliblogPrevFit, c(1, 7, 10))
@

To read documents that are highly associated with topics the \code{findThoughts} function can be used. This function will print to the user the documents highly associated with each topic.\footnote{The \code{theta} parameter in the stm object output has the posterior probability that this function uses.} Reading example documents is helpful in understanding the content of a topic and interpreting its meaning.

In this example, for expositional purposes, we restrict the length of the documents to just plot the first 250 characters. We see that Topic 1 describes the discussion of Jeremiah Wright that occurred around the most recent election. Topic 7 discusses Sarah Palin and other vice presidential candidates, with some discussion of other conservative presidential candidates such as Huckabee or Romney. Topic 10 discusses the Bush administration.

To print example documents to a graphics device \code{plotQuote} can be used. The results are displayed in Figure~\ref{fig:example}).

<<eval=TRUE,results=hide>>=
thoughts1<-findThoughts(poliblogPrevFit, texts=shortdoc, 
                        n=2, topics=1)$docs[[1]]
thoughts7 <- findThoughts(poliblogPrevFit, texts=shortdoc, 
                          n=2, topics=7)$docs[[1]]
thoughts10 <- findThoughts(poliblogPrevFit, texts=shortdoc, 
                           n=2, topics=10)$docs[[1]]
@

%manually correcting some unicode errors
<<eval=TRUE, echo=FALSE>>=
thoughts7 <- c("The Sarah Palin Digest: What We Know About McCain's Running Mate   Very little was known nationally about Alaska Gov. Sarah Palin (R) until Sen. John McCain (R-AZ) selected her as his running mate on Aug. 29. Tonight, Palin will be speaking in", "Here it is: The bio video of Sarah Palin that was supposed to air at the Republican convention last night before her speech.  You should watch it. The video, which was leaked (surprisingly) to Fox News, gives us a glimpse into how the Republicans were")
thoughts10 <- c("Karl Rove orchestrating the Bush Legacy project. President Bush's interview with ABC's Charlie Gibson this week was the first of several planned exit interviews. According to White House press secretary Dana Perino, Bush", "Flashback: Seven years ago today, Bush received Bin Laden Determined to Strike in U.S. memo.  Today marks seven years since the day President Bush received a President's Daily Brief entitled Bin Laden Determined to Strike in U.S.")
@
\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
par(mfrow = c(2, 2),mar=c(.5,.5,1,.5))
plotQuote(thoughts1, width=40, main="Topic 1")
plotQuote(thoughts7, width=40, main="Topic 7")
plotQuote(thoughts10, width=40, main="Topic 10")
@
\caption{Example documents highly associated with topic 1, 7, and 10.}
\label{fig:example}
\end{center}
\end{figure}



\subsubsection{Plotting Topic/Metadata relationships}
We also provide a rich suite of plotting functions that lets users explore the relationship between their metadata and topical prevalence or content.

\paragraph{Estimation}

Before plotting, we have to run \code{estimateEffect} in order to simulate a set of parameters which can then be plotted. These parameters include the expected proportion of a document that belongs to a topic as a function of a covariate, or a first difference type estimate, where topic prevalence for a particular topic is contrasted for two groups (e.g., liberal versus conservative). \code{estimateEffect} uses the method of composition to calculate uncertainty in the parameters fitted by the linear model of the topic proportions on the covariates.  In the below example this is run on topics 1 and 2. \code{estimateEffect} should be run and saved before plotting because it can be time intensive to calculate uncertainty estimates and/or because users might wish to plot different quantities of interest using the same simulated parameters.\footnote{The help file for this function describes several different ways for uncertainty estimate calculation, some of which are much faster than others.} The output can then be plotted. In this example we use a string variable and it is required that for this function that we first convert it into a factor variable.

<<eval=TRUE>>=
meta$rating<-as.factor(meta$rating)
prep <- estimateEffect(1:20 ~ rating+s(day),poliblogPrevFit,
         meta=meta, uncertainty = "Global")
@

The syntax of the \code{estimateEffect} function is designed so users specify the set of topics they wish to use for estimation, and then a formula for metadata of interest. After the necessary method of composition simulations are done particular estimate strategies and standard plot design features can be used by calling the \code{plot.estimateEffect} function. First, users must specify the variable that they wish to use for calculating an effect. If there were multiple variables specified in \code{estimateEffect}, then all other variables are held at their sample median. Second users must specify the type of uncertainty calculation. The default is ``global'', which will incorporate estimation uncertainty of the topic proportions into the uncertainty estimates using the method of composition. If users do not propagate the full amount of uncertainty, e.g., to  speed up computational time, they can choose \code{uncertainty="None"} which will result in narrower confidence intervals because it will not include the additional estimation uncertainty.


\paragraph{Plotting}

Users must also select the type of quantity that is to be plotted. When the covariate of interest is binary, or users are interested in a particular contrast, the method="difference" option will plot the change in topic proportion shifting from one specific value to another. Figure~\ref{fig:difference} gives an example. For factor variables, users may wish to plot the marginal topic proportion for each of the levels ("pointestimate").


\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plot.estimateEffect(prep, covariate = "rating", topics = c(1, 7, 10),
        model=poliblogPrevFit, method="difference",
        cov.value1="Liberal",cov.value2="Conservative",
        xlab="More Conservative ... More Liberal",
        main="Effect of Liberal vs. Conservative",
        xlim=c(-.1,.1), labeltype = "custom",
        custom.labels = c('Jeremiah Wright', 'Sarah Palin', 
                          'Bush Presidency'))
@
\caption{Graphical Display of Topical Prevalence Contrast.}
\label{fig:difference}
\end{center}
\end{figure}

We see Topic 1 is strongly used by conservatives compared to liberals, while Topic 7 is close to the middle but still conservative-leaning. Topic 10, the discussion of Bush, was largely associated with liberal writers, which is in line with the observed trend of conservatives distancing from Bush after his presidency.

Notice how the function makes use of standard labeling options available in the native plot() function. This allows the user to customize labels and other features of their plots. We note that in the package we leverage generics for the plot functions. As such, one can simply use \code{plot} instead of writing out the full extension (e.g., in Figure~\ref{fig:difference} one could use \code{plot()} instead of \code{plot.estimateEffect}). For expositional purposes in this vignette, we include the entire extension.

When users have variables that they want to treat continuously, users can choose between assuming a linear fit or using splines. In the previous example, we allowed for the day variable to have a non-linear relationship in the topic estimation stage. We can then plot its effect on topics. In Figure~\ref{fig:spline}, we plot the relationship between time and the vice presidential topic, topic 7.  The topic peaks when Sarah Palin became John McCain's running mate at the end of August in 2008.

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plot.estimateEffect(prep, "day", method="continuous", topics=7, model=z,
printlegend=FALSE, xaxt="n", xlab="Time (2008)")
monthseq <- seq(from=as.Date("2008-01-01"), 
                to=as.Date("2008-12-01"), by="month")
monthnames <- months(monthseq)
axis(1, 
     at=as.numeric(monthseq)-min(as.numeric(monthseq)), 
     labels=monthnames)
@
\caption{Graphical Display of Topic Prevalence. Topic 7 prevalence is plotted as a smooth function of day, holding rating at sample median.}
\label{fig:spline}
\end{center}
\end{figure}


\subsubsection{Topical Content}
We can also plot the influence of covariates included in topical content. A topical content variable allows for the vocabulary used to talk about a particular topic to vary. First, the STM must be fit with a variable specified in the content option. In the below example, ratings serves this purpose. It is important to note that this is a completely new model, and so the actual topics may differ in both content and numbering compared to the previous example where no content covariate was used.

<<eval=FALSE>>=
poliblogContent <- stm(out$documents,out$vocab,K=20,
        prevalence =~ rating+ s(day), content=~rating,
        max.em.its=75, data=out$meta,seed=5593453)
@

Next, the results can be plotted using the \code{plot.STM(,type="perspectives")} function.  This functions shows which words within a topic are more associated with one covariate value versus another. In Figure~\ref{fig:perp}, vocabulary differences by ratings is plotted for topic 10. Topic 10 is related to Guantanamo. Its top words were ``guantanamo, techniqu, interrog, cia, detaine, tortur, detent.'' However, Figure~\ref{fig:perp} lets us see how liberals and conservatives talk about this topic differently. In particular, liberals emphasized ``torture'' whereas conservatives emphasized how the detainees were ``terrorists.''\footnote{At this point you can only
 have a single variable as a content covariate, although that variable can have any number of groups. It cannot be continuous. Note that the computational cost of this type of model rises quickly with the number of groups and so it may be advisable to keep it small.}

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plot.STM(poliblogContent,type="perspectives", topics=10)
@
\caption{Graphical Display of Topical Perspectives.}
\label{fig:perp}
\end{center}
\end{figure}

This function can also be used to plot the contrast in words across two topics.\footnote{This plot calculates the difference in probability of a word for the two topics, normalized by the maximum difference in probability of any word between the two topics.} To show this we go back to our original model that did not include a content covariate and we contrast topic 9 (Iraq war) and 10 (Bush presidency). We plot the results in Figure~\ref{fig:perp2}.

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plot.STM(poliblogPrevFit,type="perspectives", topics=c(9, 10))
@
\caption{Graphical Display of Topical Contrast between Topics 9 and 10.}
\label{fig:perp2}
\end{center}
\end{figure}

The cloud function can also be used to visualize topics.  For example, Figure \ref{fig:cloud} displays a word cloud of the most probable words in a topic related to the vice presidential candidates in the 2008 election.

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
cloud(poliblogPrevFit, topic=7)
@
\caption{WordCloud Display of Topics.}
\label{fig:cloud}
\end{center}
\end{figure}

\subsubsection{Covariate Interactions}

Another modification that is possible in this framework is to allow for interactions between covariates such that we allow for one variable to ``moderate'' the effect of another variable. In this example, we re-estimated the STM to allow for an interaction between day (entered linearly) and ratings. Then in \code{estimateEffect()} we include the same interaction. This allows us in \code{plot.estimateEffect} to have this interaction plotted. We display the results in Figure~\ref{fig:spline2} for topic 1 (Jeremiah Wright). We observe that liberals never really talked about this topic, whereas for conservatives this was initially high and then declined.\footnote{Note that the ability to plot interactions is somewhat limited and only supports interactions with a binary effect modification covariate and does not support interactions with a spline.}

<<eval=FALSE>>=
poliblogInteraction <- stm(out$documents,out$vocab,K=20,
        prevalence =~ rating* day, max.em.its=75,
        data=out$meta,seed=5926696)
@

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
prep <- estimateEffect(c(1) ~ rating*day, poliblogInteraction,
        metadata=meta, uncertainty="None")

plot.estimateEffect(prep, covariate="day", model=poliblogInteraction,
        method="continuous",xlab="Days", moderator="rating",
        moderator.value="Liberal", linecol="blue", ylim=c(0,.08), 
        printlegend=F)

plot.estimateEffect(prep, covariate="day", model=poliblogInteraction,
        method="continuous",xlab="Days", moderator="rating",
        moderator.value="Conservative", linecol="red", add=T, 
        printlegend=F)
legend(0,.08, c("Liberal", "Conservative"), 
       lwd=2, col=c("blue", "red"))
@
\caption{Graphical Display of Topical Content allowing for interaction between day of blog post and liberal versus conservative interaction. Topic 1 prevalence is plotted as linear function of day, holding the rating at either 0 (Liberal) or 1 (Conservative). Were other variables included in the model, they would be held at their sample medians.}
\label{fig:spline2}
\end{center}
\end{figure}


More details are available in the help file for this function.\footnote{An additional option is the use of local local regression (loess). In this case, because multiple covariates are not possible a separate function is required, \code{plotTopicLoess}, which contains a help file for interested users.}.


\subsubsection{Corpus level plotting}

Corpus level visualization can be done in several different ways. The first relates to the expected proportion of the corpus that belongs to each topic. This can be be plotted using \code{plot.stm(,type="summary")}. An example from the political blogs data is given in Figure~\ref{fig:summary}. We see, for example, that the Sarah Palin/Vice President topic (7) is actually a relatively minor proportion of the discourse.  The most common topic, topic 3, is a general topic full of words that bloggers commonly use, and therefore is not very interpretable. The words printed are the top three words associated with the topic.

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plot.STM(poliblogPrevFit,type="summary", xlim=c(0,.4))
@
\caption{Graphical Display of Estimated Topic Proportions.}
\label{fig:summary}
\end{center}
\end{figure}

Users can also plot features of the corpus as a whole. First, the Structural Topic Model permits correlations between topics. Positive correlations between topics indicate that both topics are likely to be discussed within a document. These can be visualized using \code{plot.topicCorr()}. The user can specify a correlation threshold.  If two topics are correlated above that threshold, than those two topics are considered linked.  After calculating the links between topics, \code{plot.topicCorr} produces a layout of topic correlations using a force-directed layout algorithm. \code{plot.topicCorr} has several options that are described in the help file. We can use the correlation graph to observe the connection between such as topics 9 (Iraq War) and 10 (Bush).

<<eval=TRUE>>=
mod.out.corr<-topicCorr(poliblogPrevFit)
@

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plot.topicCorr(mod.out.corr)
@
\caption{Graphical Display of Topic Correlations.}
\label{fig:correlations}
\end{center}
\end{figure}


\subsection{Diagnostic Tests}
Topic estimation is fundamentally imprecise, as estimation in topic model space requires both an a priori number of topics input by the user, and furthermore an optimization in a space with multiple solutions. Due to the intractability underlying the computation of topic models, we rely on external analytics of our model to understand its unique tradeoffs between competing parameters.  The STM package contains a variety of tools that can be used to evaluate the quality of the model as well as the user's choice of number of topics and of metadata selected for inclusion.
 

\subsubsection{1. Post-estimation Permutation Checks}
Any statistical procedure can be abused and STM is no different. One concern that we have heard is that users will simply search out covariate topic relationships that are the strongest. A related concern is the concern that by combining the measurement model with the estimation of an effect, the user is `baking in' the conclusion. In the appendix of \cite{ajps} we address this concern using both a simulation and also  a permutation test approach.  We have built in a function for conducting permuation tests using binary prevalence covariates.\footnote{Future work could extend this to other types of covariates.} The \code{permutationTest} function takes a formula containing a single binary covariate (and optionally other controls) and runs a permutation test where, rather than using the true assignment, the covariate is randomly assigned to a document with probability equal to its empirical probability in the data.
After each shuffle of the covariate the same STM model is estimated at different starting values using the same initialization procedure as the original model, and the effect of the covariate across topics is calculated.
Next the function records two quantities of interest across this set of ``runs" of the model. The first records the absolute maximum effect of the permuted covariate across all topics.
The second records the effect of the (permuted) covariate on the topic in each additional stm run which is estimated to be the topic closest to the topic of interest. The object returned from \code{permutationTest} can then be passed to \code{plot.STMpermute} for plotting.

\subsubsection{2. Checks for Multi-modality}
Another diagnostic that should be completed while running the STM is checking to see how multi-modal the model of interest is.  We provide a suite of methods to assess multi-modality, and we refer the reader to \citet{robertsnavigating} for an explanation of all of them.  For purposes of example, we check how robust the covariate effects we explore earlier are to multimodality within the four original models that we ran.  The function \code{multiSTM} aligns topics across models.  The function \code{plot.MultimodDiagnostic} plots the effects across topics and models.  As you can see in Figure \ref{fig:multi_poliblog}, we find that only some of topics  in our first STM model are robust across models.  If the topic of interest is not robust across models, we suggest looking more closely at the topic's most probable words to try to understand where these differences orginate.

\begin{figure}
  \centering
  \includegraphics[scale=.45]{multimod.png}
  \caption{Plot of Liberal-Conservative differences effect across four runs.}\label{fig:multi_poliblog}
\end{figure}

\subsubsection{3. Held-out likelihood estimation}
Sometimes users will want to compare model specifications to see how well the model does predicting words within the document.  The \texttt{stm} package contains two different functions to aid with held-out likelihood estimation.  Held-out likelihood estimation is the estimation of the probability of words appearing within a document when those words have been removed from the document in the estimation step \citep{blei2003latent}.  Similar to cross-validation, when some of the data is removed from estimation and then later used for validation, the held-out likelihood helps the user assess the model's prediction performance.

We provide two different functions for the user to complete heldout likelihood estimation.  The first,  \code{make.heldout} produces a document set where some of the words within the documents have been removed.  The user can then run a STM model on the documents that have the missing words.  The second, \code{eval.heldout}, evaluates the heldout likeihood for missing words based on the model run on the heldout documents.

\subsubsection{4. Residuals Checks}
Users can also test the assumptions of the model within the package through the function \code{residuals}.  This function implements residual checks described in Section 4.2 of \citet{taddyestimation}, testing for overdispersion of the variance of the multinomial within the data generating process of STM.  As described in \citet{taddyestimation}, if the residuals are overdispered, it could be that more topics are needed to soak up some of the extra variance.  While no fool-proof method has been developed to choose the number of topics, both the residual checks and held-out likelihood estimation can be useful for indications of the number of topics to choose.

In addition to these functions one can also explore if there words that are extremely highly associated with a single topic via the \code{checkBeta} function.




\section{Changing Basic Estimation Defaults}
In this section we overview how to change basic defaults in estimation.  We start by discussing how to chose among different methods for intializing model parametes.  We then discuss how to set and evaluate convergence criteria.  Next we overview a method for accelerating convergence in document sets with many documents. Finally we discuss some variations on content covariate models which allow the user to control model complexity.

\subsection{Initialization}
As with most topic models, the objective function maximized by STM is multimodal.  This means that the way we choose the starting values for the variational EM algorithm can affect our final solution.  We provide three methods of initialization accessed using the argument \code{init.type}, Latent Dirichlet Allocation via collapsed Gibbs sampling (\code{init.type="LDA"}), a Spectral algorithm for Latent Dirichlet Allocation (\code{init.type="Spectral"}), and random starting values (\code{init.type="Random"}).  

LDA is the default option and uses a few passes of collapsed Gibbs sampling to initialize the algorithm. The exact parameters for this initialization can be set using the argument \code{control}.The Spectral option initializes using a moment based estimator for LDA due to \citet{arora2012practical}. In contrast to the LDA and random initializations this approach is deterministic.  It performs extremely well particularly for larger document sets.  Because the spectral algorithm needs to form a square matrix with dimensions of the lenght of the vocabulary, it is best used in settings where the vocabulary is under 5000 terms.  Finally the random algorithm draws the initial state from a Dirichlet distribution.  This is included primarily for completeness and in general th other two strategies should be preferred.  \citet{robertsnavigating} provides details on these initialization methods and provides a study of their performance.  In general Spectral outperforms LDA which in turn outperforms Random initialization.

Each time the model is run the random seed is saved in the output object under \code{settings$seed}.  This can be passed to the \code{seed} argument of \code{stm} to replicate the same starting values.


\subsection{Convergence Criteria}
Estimation in the STM proceeds by variational EM.  Convergence is controlled by relative change in the variational objective.  Denoting by $\ell_t$ the approximate variational object at time $t$, convergence is declared when the quantity $\ell_t - \ell_{t-1}/$abs($\ell_{t-1}$) drops below tolerance.  The default tolerance is 1e-5 and can be changed using the \code{emtol} argument.

The argument \code{max.em.its} sets the maximum number of iterations.  If this threshold is reached before convergence is assessed a message will be printed to the screen.  The default of 500 iterations is simply a general guideline.  A model which fails to converged can be restarted using the \code{model} argument in \code{stm}.  See the documentation for \code{stm} for more information.

The default is to have the status of iterations print to the screen. The \code{verbose} option turns printing to the screen on and off.

During the E-step the algorithm prints one dot for every $1\%$ of the corpus it completes and announces completion along with timing information.  Printing for the M-Step depends on the algorithm being used.  For models without content covariates M-step estimation should be nearly instantaneous.  For models with content covariates and the algorithm prints dots to indicate progress.  The exact interpretation of the dots differs with the choice of model (see the help file for more details).

By default every 5th iteration will print a report of top topic and covariate words.  The \code{reportevery} option sets how often these reports are printed.

Once a model has been fit, convergence can easily be assessed by plotting the variational bound as in Figure \ref{fig:converge}. 

\begin{figure}
\begin{center}
<<eval=TRUE,fig=TRUE>>=
plot(poliblogPrevFit$convergence$bound,type="l", 
     ylab="Approximate Objective",
     main="Convergence")
@
\caption{Graphical Display of Convergence.}
\label{fig:converge}
\end{center}
\end{figure}

\subsection{Accelarating Convergence}
When the number of documents is large convergence in topic models can be slow.  This is because each iteration requires a complete pass over all the documents before updating the global parameters.  To accelerate convergence we can split the documents into several equal sized blocks and update the global parameters after each block.  The option \code{ngroups} specifies the number of blocks, and setting it equal to an integer greater than one turns on this functionality. 

Note that setting the \code{ngroups} value to a large number can dramatically increase the memory requirements of the function.  Thus as the number of blocks is increased we are trading off memory for computational efficiency.  

\subsection{SAGE}
The Sparse Additive Generative (SAGE) model conceptualizes topics as sparse deviations from a corpus-wide baseline \citep{eisenstein2011sparse}.  While computationally more expensive this can sometimes produce higher quality topics .  Whereas LDA will tend to assign rare words exclusively to one topic, the regularization of the SAGE model ensures that words only load onto topics when they have sufficient counts to overwhelm the prior.  In general this means that SAGE topics will tend to have fewer words that distinguish them from other topics, but those words are more likely to be meaningful.  Importantly for our purposes the SAGE framework makes it straightforward to add covariate effects into the content portion of the model.

\paragraph{Covariate-Free SAGE}
While SAGE topics are enabled automatically when using a covariate in the content model they can also be used even without covariates.  To activate SAGE topics simply set the option \code{LDAbeta=FALSE}.

\paragraph{Covariate-Topic Interactions}
By default when a content covariate is included in the model, we also include covariate-topic interactions.  In our political blog corpus for example this means that the probability of observing a word from a Conservative blog in Topic 1 is formed by combining the baseline probability, the Topic 1 component, the Conservative component and the Topic 1 - Conservative interaction component.

Users can turn off interactions by specifying the option \code{interactions=FALSE}.  This can be helpful in settings where there isn't sufficient data to make reasonably inferences about all the interaction parameters.  It also reduces the computational intensity of the model.

\section{Alternate Priors}
In this section we overview options for altering the prior structure in the \code{stm}  function.  We highlight the alternatives and provide intuition for the properties of each option. We have chosen the default settings to represent the choices that we expect will perform  the best in the majority of cases and thus changing these settings should only be necessary if the defaults are not performing well.

\subsection{Changing Estimation of Prevalence Covariate coefficients}
The user can choose between two options: "Pooled" and "L1".   The difference between these two is that the "L1" option can induce sparsity in the coefficients (i.e. many are set exactly to zero) while the "Pooled" esitmator is computationally more efficient.  In practice we recommend the default "Pooled" estimator unless the prevalence covariates are very high dimensional (such as a factor with hundreds of categories).

"Pooled" is the default option and estimates a model where the coefficients on topic prevalence have a zero-mean Normal prior with variance given a broad inverse-gamma hyperprior.  This provides moderate shrinkage towards zero but does not induce sparsity.

You can also choose \code{gamma.prior="L1"} which uses the \code{glmnet} package \citep{friedman2010regularization} to allow for grouped penalties between the L1 and L2 norm. In these settings we estimate a regularization path and then select the optimal shrinkage parameter using a user-tunable information criterion. By default selecting the L1 option will apply the L1 penalty selecting the optimal shrinkage parameter using AIC. The defaults have been specifically tuned for the STM but almost all the relevant arguments can be changed through the \code{control} argument. Changing the \code{gamma.enet} parameter by specifying \code{control=list(gamma.enet=.5)} allows the user to choose a mix between the L1 and L2 norms. When set to 1 (as by default) this is the lasso penalty, when set to 0 its the ridge penalty. Any value in between is a mixture called the elastic net.

\subsection{Changing Covariance Matrix Prior}
The \code{sigma.prior} argument is a value between 0 and 1 defaulting to 0.  The update for the covariance matrix is formed by taking the convex combination of the diagonalized covariance and the MLE with weight given by the prior.  Thus by default we are simply maximizing the likelihood.  When \code{sigma.prior=1} this amounts to setting a diagonal covariance matrix.  This argument can be useful in settings where topics are at risk of becoming too highly correlated.  However, in extensive testing we have come across very few cases where this was needed.

\subsection{Changing the Content Covariate Prior}
The \code{kappa.prior} option provides two sparsity promoting priors for the content covariates.  The default is \code{kappa.prior="L1"} and uses \code{glmnet} and the distributed multinomial formulation of \citet{taddy2013distributed}.  The core idea is to decouple the update into a sequence of independent L1-regularized poisson models with plugin estimators for the document level shared effects.  See \citet{STMEdo} for more details on the estimation procedure. The regularization parameter is set automatically as detailed in the \code{stm} help file.

To maintain backwards compatability we also provide estimation using a scale mixture of Normals where the precisions $\tau$ are given improper Jeffreys priors $1/\tau$. This option can be accessed by setting \code{kappa.prior="Jeffreys"}.  We caution that this can be much slower than the default option.

There are over a dozen additional options documented in \code{stm} for altering additional components of the prior, most of them focusing on the content covariate model.

\section{Conclusion}

The  \pkg{stm} package provides a flexible integration of document metadata and topic modeling. This vignette provides an overview of use and features. We encourage users to consult the extensive help files for more details, as well as read the companion papers that illustrate the application of this method.

The development of the structural topic model is ongoing, and future research promises a variety of tools for better models and more user-friendly results.  Software for the visualization of the STM is ongoing, with development of a web browser-based visualization tool underway. Such interactive visualizations will allow for further exploration of topics and the interactions between topics and covariates.

Furthermore, in performing these intensive machine learning computations, there are always gains in efficiency to be had, both in theoretical optimality and in applied programming practice.  The STM is undergoing constant streamlining and revision towards faster, more optimal computation.  As corpus sizes increase, the STM will also increase in capacity to handle more documents and more varied metadata.

\clearpage
\pdfbookmark[1]{References}{References}
\bibliography{vignetteBib}


\end{document}

